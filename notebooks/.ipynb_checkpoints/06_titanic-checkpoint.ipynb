{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Project: Exploratory Data Analysis on the Titanic Dataset\n",
    "This exercise comes from the [DSFM code-bank](https://github.com/dsfm-org/code-bank/blob/master/demos/eda-titanic/eda-titanic.ipynb) repository of code. It has been adapted here to the Python quickstart workshop.\n",
    "\n",
    "## Overview\n",
    "In this demo, we will explore the likelihood of surviving the sinking of the RMS Titanic. The Titanic hit a iceberg in 1912 and quickly sank, killing 1,502 out of the 2,224 passengers and crew on board. One most important reason so many people died was that there were not enough lifeboats to serve everyone. Accordingly, it has also been frequently noted that the most **_likely_** people to survive the disaster were women, children, and members of the upper-class. Let's see if that is true.\n",
    "\n",
    "The Titanic case is a classic problem in data science, and it is still an ongoing [Kaggle competition](https://www.kaggle.com/c/titanic). There are many other examples of the Titanic dataset in introductory statistics and Data Science courses, so we also encourage you to look around and see how others have approached the problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/9/95/Titanic_sinking%2C_painting_by_Willy_St%C3%B6wer.jpg\" width=\"500\" height=\"500\" align=\"center\"/>\n",
    "\n",
    "Image source: https://upload.wikimedia.org/wikipedia/commons/9/95/Titanic_sinking%2C_painting_by_Willy_St%C3%B6wer.jpg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "We will conduct our EDA and visualization analysis in three parts:\n",
    "\n",
    "  1. Analyze and visualize base-rates \n",
    "  2. Calculate new predictors that can help the analysis ('Feature Engineering)\n",
    "  3. Visualize advanced data characteristics\n",
    "  \n",
    "To prepare, let's first review the data tools, visualization tools, and actual data for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Structures for Python\n",
    "\n",
    "There are three basic options for loading and working with data in Python:\n",
    "\n",
    "  * Pure `Python 3.x`\n",
    "  \n",
    "    In this approach, you load data directly into \"pure\" Python data objects, such as lists, sets, and dictionaries (or nested hiearchies of such objects, such as lists-within-lists, lists-within-dicts, dicts-within-dicts, and so on). Although operation on \"pure Python\" objects can be slow, it also is extremely flexible.\n",
    "    \n",
    "  * `NumPy`\n",
    "  \n",
    "    The basic data structures for holding arrays, vectors, and matrices of data are provided by a core package called `NumPy`. NumPy also has a set of linear algebra and numerical fuinctions, but in general such functions are now provided by another package called `scipy` (for scientific computing) and numerical computation is usually done there. `NumPy` has been optimized to run with primitive routines written in `C` (or even `fortran`) and so is orders-of-magnitude faster-running than doing calculations with pure Python. Nevertheless, the data access, subscripting, and slicing of elements for `NumPy` still conforms to the same syntax as pure Python.\n",
    "    \n",
    "  * `pandas`\n",
    "  \n",
    "    Most applied Data Science projects (that fit into memory/RAM), now use an \"Excel-like\" package called `pandas`. Pandas stores data in objects called **dataframes**. Dataframes will become the central type of data object for your work in Data Science (much as Excel Spreadsheets often were for Business Analysts). Dataframes provide many different properties and methods that help you to work with your data more effectively. We will use `pandas` for most of the examples and problems in the class.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization for Python\n",
    "\n",
    "There are many ways to visualize data and results in Python. Sometimes that is a good thing - and sometimes it is a bad thing, for there are many ways to do it. Eventually you will want to learn multiple methods, as data scientists often use many different libraries. The following are the most common libraries, with links to their documentation:\n",
    "\n",
    "  * `pandas` https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html\n",
    "  \n",
    "    If you are in a hurry, it also is possible to generate many simple plots directly from the `pandas` library and `dataframe` object. This is a good option when you are moving fast and all you need to so is see a simple histogram or trend line and you already have your data in a pandas dataframe.  \n",
    "  \n",
    "  * `seaborn` https://seaborn.pydata.org/\n",
    "  \n",
    "    Seaborn is a simplified and better looking interface that sits on top of the standard `matplotlib` library. Seaborn is often used because it looks great, but also gives you all the ability to go into `matplotlib` to customize graphs for a particular need.\n",
    "  \n",
    "  * `plotly` and `plotly_express` https://www.plotly.express/   \n",
    "  \n",
    "    The commercial package `plotly` is a comprehensive toolkit for building interactive, D3 and WebGL charts. Interactive graphs are particularly good for online (web-based) dashboards. As you can see in the plot of Python visualization options below, `plotly` is quickly rising in popularity. To use plotly, however, you need to sign up for a [plotly account](https://plot.ly/python/) and you need an active internet connection. We won't need all of the features in plotly, however, and so we will develop examples using just `plotly_express`. Plotly express is a free, local-to-your-machine, and easier-to-use, API to the plotly service. [See here for documentation](https://www.plotly.express/plotly_express/) for the plotly express API.  \n",
    "    \n",
    "  * `matplotlib` and `pyplot` https://matplotlib.org/  \n",
    "  \n",
    "    Matplotlib is the core package for graphing in Python, and many other packages build on top of it (i.e., pyplot, pandas, and seaborn). The `matplotlib` object model can be somewhat confusing, which often means writing many lines of code and hours of debugging. To help, matplotlib also comes with an interface library called `pyplot` ([documentation here](https://matplotlib.org/api/pyplot_api.html)) that mimics the MatLab approach to graphing (helpful for many engineers). In general, however, `pyplot` has now been supplanted by the other options above. Although it is faster to get started with plotting by using one of the other options above, eventually you will find that you often need to return to matplotlib in order to \"tweak\" a layout or to work with more complicated graphs.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Titanic Dataset\n",
    "\n",
    "The data is taken from the [Kaggle Titanic Competition](https://www.kaggle.com/c/titanic). It is split between a \"training\" dataset (where you know the actual outcome) and a \"testing\" dataset (where you do not know the outcome). If we were actually competing in the Kaggle competition, then we would be trying to predict the unknown testing cases and submitting our predictions to Kaggle to see if we could win. But in this case, the objective is simply to get you started with Python and to familiarize you with the basic data structures and graphing libraries of the Data Science stack. We therefore will ignore the testing dataset and work only with the training data.\n",
    "\n",
    "All of the data that you will need for this demo is in the `titanic.csv` file, located within the same directory as this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't know very much about the 891 passengers in the training dataset. The following features are available.\n",
    "\n",
    "  Feature name | Description  |\n",
    "  -------- | -------------- |\n",
    "  Survived | Target variable, i.e. survival, where 0 = No, 1 = Yes |\n",
    "  PassengerId | Id of the passenger |\n",
    "  Pclass   | Ticket class, wher 1 = 1st, 2 = 2nd, 3 = 3rd |\n",
    "  Name     | Passenger name |\n",
    "  Sex      | Sex |\n",
    "  Age      | Age in years |\n",
    "  SibSp    | Num of siblings or spouses aboard the Titanic |\n",
    "  Parch    | Num of parents or children aboard the Titanic |\n",
    "  Ticket   | Ticket number, i.e. record ID |\n",
    "  Fare     | Passenger fare |\n",
    "  Cabin    | Cabin number |\n",
    "  Embarked | Port of Embarkation, where C = Cherbourg, Q = Queenstown, S = Southampton |\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Special Notes**\n",
    "\n",
    "  * **Pclass**: A proxy for socio-economic status (SES): 1st = Upper class; 2nd = Middle class; 3rd = Lower class.  \n",
    "  * **Age**: Age is fractional if less than 1.  If the age is estimated, is it in the form of xx.5  \n",
    "  * **SibSp**: The dataset defines family relations as: Sibling = brother, sister, stepbrother, stepsister; Spouse = husband, wife (mistresses and fianc√©s were ignored)  \n",
    "  * **Parch**: The dataset defines family relations as: Parent = mother, father; Child = daughter, son, stepdaughter, stepson; Some children travelled only with a nanny, therefore parch=0 for them.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 0**: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all import statements at the top of your notebook -- import some basic and important ones here\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn           as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fname = 'data/titanic_train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 1**: Analyze Base Rates and EDA\n",
    "\n",
    "The most basic \"model\" to run for any problem is the \"average\" (or \"base rate\") outcome. Before we work on more complicated models, it is a good idea to understand how a very simple heuristic will perform -- and then you can determine how much a more complicated model really improves the predictions. Your first model may be too simple and therefore highly \"biased\" (i.e., systematically low or systematically high for different groupings or levels within the data); but your simple model should also not vary much if/when we pull a new sample from the same underlying population/data generating function.  \n",
    "\n",
    "We will therefore conduct some **_exploratory data analysis_** (\"**EDA**\") and **_visualization_** in this demo to understand the distribution of the outcome for this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open and Inspect the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open dataset with Pandas\n",
    "data = pd.read_csv(fname)\n",
    "\n",
    "# Count rows and coumns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are some useful steps for understanding and plotting the data in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the data (variable names, count non-missing values, review variables types)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the \"head\" of the dataset\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate summary statistics (mean, min, max, std, etc.) for all variables and transpose the output\n",
    "\n",
    "data.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing data per feature\n",
    "\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot missing data  (Hint: use a seaborn heatmap to see distribution of isnull() for the dataframe\n",
    "\n",
    "sns.heatmap(data.isnull(), cbar=False, cmap=\"YlGnBu_r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Survived'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aside: selecting columns in pandas\n",
    "\n",
    "To select columns of a dataframe, we pass the column name as a string in square brackets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass the column name as a string in square brackets\n",
    "\n",
    "data['Survived'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Survival (the basic outcome)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of Survivals and Deaths\n",
    "\n",
    "survivals = sum(data['Survived'])\n",
    "deaths = len(data[data['Survived'] == False])\n",
    "assert survivals + deaths == len(data)     # not necessary, but this should be true\n",
    "assert survivals + deaths == data.shape[0] # not necessary, but this also should be true\n",
    "print('Survivals: ', survivals)\n",
    "print('Deaths: ', deaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The base-rate likelihood of survival: ', survivals/(survivals+deaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of \"Survived\" using Pandas \n",
    "# The fastest way - this just uses the pandas dataframe  \n",
    "\n",
    "data['Survived'].hist()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a histogram of \"Survived\" using Seaborn\n",
    "# The fastest way is to reference Column Names as Properties of the Dataframe\n",
    "\n",
    "sns.countplot(data[\"Survived\"])\n",
    "\n",
    "# Note that another, \"safe\" way to code is to pass parameters by name\n",
    "# sns.countplot(x='Survived', data=data) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Base-Rate Outcomes, by Condition\n",
    "\n",
    "Again, let's start by calculating the frequency (or \"base rate\") of the outcome variable for different conditions of interest. You can do this most easily by using the `.crosstab()` function from the `pandas` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pandas crosstab() function to count outcomes by condition\n",
    "\n",
    "pd.crosstab(data['Pclass'], data['Survived'], margins=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now show the totals (so you can calculate a conditional marginal base rate).\n",
    "\n",
    "pd.crosstab(data['Pclass'], data['Survived'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the style() function (chain it onto the end of prior code) to also overlay a heatmap \n",
    "# i.e., you can still do this in one line of code with .style.background_gradient()\n",
    "\n",
    "pd.crosstab(data['Pclass'], data['Survived'], margins=True).style.background_gradient()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you can't read the results, select your own color scheme\n",
    "\n",
    "pd.crosstab(data['Pclass'], data['Survived'], margins=True).style.background_gradient(cmap='autumn_r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do a \"three-way\" crosstab for Class, Sex, Survival\n",
    "\n",
    "pd.crosstab([data['Sex'], data['Survived']], data['Pclass'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the pandas dataframe to plot a histogram of Age \n",
    "\n",
    "data['Age'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Increase the number of histogram bins\n",
    "\n",
    "data['Age'].hist(bins = 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn to plot the kernel density (a kdeplot) for Age\n",
    "\n",
    "facet = sns.FacetGrid(data, aspect = 3)\n",
    "facet.map(sns.kdeplot, 'Age', shade = True)\n",
    "facet.set(xlim = (0, data['Age'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Seaborn to plot the kernel density of Fare\n",
    "\n",
    "facet = sns.FacetGrid(data, aspect=4)\n",
    "facet.map(sns.kdeplot,'Fare', shade=True)\n",
    "facet.set(xlim = (0, data['Fare'].max()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redo the above in just 1 line of code, but show both a frequency histogram of counts, \n",
    "# and a kernel density of the probability density function\n",
    "\n",
    "# There usually is a simple way to do it with seaborn...\n",
    "\n",
    "facet = sns.distplot(data['Fare'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to plot a histogram of Survived, separated by Class\n",
    "# Hint: figsize is defined in inches\n",
    "\n",
    "lived = data[data['Survived'] == 1]['Pclass'].value_counts()\n",
    "died  = data[data['Survived'] == 0]['Pclass'].value_counts()\n",
    "df = pd.DataFrame([lived, died])\n",
    "df.index = ['Lived', 'Died']\n",
    "df.plot(kind = 'bar', stacked=True, figsize=(12, 5), title='Survival by Social Economic Class (1st, 2nd, 3rd)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use seaborn barplots to plot Survival as a Function of Class\n",
    "\n",
    "sns.barplot(x='Pclass', y='Survived', data=data)\n",
    "plt.ylabel(\"Survival Rate\")\n",
    "plt.title(\"Survival as function of Pclass\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas to plot a histogram of Survived, separated by Sex\n",
    "\n",
    "lived = data[data['Survived'] == 1]['Sex'].value_counts()\n",
    "died  = data[data['Survived'] == 0]['Sex'].value_counts()\n",
    "df = pd.DataFrame([lived, died])\n",
    "df.index = ['Lived', 'Died']\n",
    "df.plot(kind = 'bar', stacked = True, figsize = (12, 5), title = 'Survival by Gender');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Pandas (with matplotlib customization to make it look good) to draw a pie chart of Survival by Sex\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1,2,figsize=(16,7))\n",
    "data['Survived'][data['Sex'] == 'male'].value_counts().plot.pie(ax = ax1)\n",
    "data['Survived'][data['Sex'] == 'female'].value_counts().plot.pie(ax = ax2, colors = ['C1', 'C0']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use some matplotlib customization to make your previous plot look cool \n",
    "\n",
    "f, ax = plt.subplots(1, 2, figsize = (16, 7))\n",
    "data['Survived'][data['Sex'] == 'male'].value_counts().plot.pie(explode=[0,0.2], autopct='%1.1f%%', ax = ax[0], shadow = True)\n",
    "data['Survived'][data['Sex'] == 'female'].value_counts().plot.pie(explode=[0,0.2], autopct='%1.1f%%', ax = ax[1], shadow = True, colors = ['C1', 'C0'])\n",
    "ax[0].set_title('Survived (male)')\n",
    "ax[1].set_title('Survived (female)');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a seaborn facet grid to jointly examine Sex, Class, and Survival\n",
    "\n",
    "g = sns.FacetGrid(data, row = 'Sex', col = 'Pclass', hue = 'Survived', margin_titles = True, height = 3, aspect = 1.1)\n",
    "g.map(sns.distplot, 'Age', kde = False, bins = np.arange(0, 80, 5), hist_kws = dict(alpha=0.6))\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the disribution of Fare as a function of Pclass, Sex and Survived\n",
    "\n",
    "g = sns.FacetGrid(data, row = 'Sex', col = 'Pclass', hue = 'Survived', margin_titles = True, height = 3, aspect = 1.1)\n",
    "g.map(sns.distplot, 'Fare', kde = False, bins = np.arange(0, 550, 50), hist_kws = dict(alpha = 0.6))\n",
    "g.add_legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADVANCED: combine histograms for many variables related to survival into one composite figure\n",
    "\n",
    "R = 2\n",
    "C = 3\n",
    "fields = ['Survived', 'Sex', 'Pclass', 'SibSp', 'Parch', 'Embarked']\n",
    "\n",
    "fig, axs = plt.subplots(R, C, figsize = (12, 8))\n",
    "\n",
    "for row in range(0, R):\n",
    "    for col in range(0, C):  \n",
    "        i = row * C + col       \n",
    "        ax = axs[row][col]\n",
    "        sns.countplot(data[fields[i]], hue = data[\"Survived\"], ax = ax)\n",
    "        ax.set_title(fields[i], fontsize = 14)\n",
    "        ax.legend(title = \"survived\", loc = 'upper center') \n",
    "        \n",
    "plt.tight_layout()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 2**: Feature Engineering\n",
    "\n",
    "Using domain knowledge, we can create new features that might improve performance of our model at a later stage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the leading \"title\" from the passenger name, and summarize (count) the different titles \n",
    "\n",
    "data['Title'] = data['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\n",
    "data['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Part 3**: Explore Swarm and Violin Plots\n",
    "\n",
    "Swarm and violin plots show the same data as plots of counts and/or density distributions (graphs you did earlier), but they also happen to look very cool and can also draw attention to details that you do not see in other plots. If you have extra time, try to make a few of these below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some constants (such as PALLET and FIGSIZE) so that all of your figures look consistent\n",
    "\n",
    "PALETTE = [\"orange\" , \"lightblue\"]  # you can set a custom palette with a simple list of named color values\n",
    "FIGSIZE = (13, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a seaborn \"swarmplot\" to examine survival by age and class.\n",
    "\n",
    "fig, ax = plt.subplots(figsize = FIGSIZE)\n",
    "sns.swarmplot(x = 'Pclass', y = 'Age', hue = 'Survived', dodge = True, data = data, palette = PALETTE, size = 7, ax = ax)\n",
    "plt.title('Survival Events by Age and Class ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a seaborn \"violinplot\" to examine survival by age and class.\n",
    "\n",
    "fig, ax = plt.subplots(figsize = FIGSIZE)\n",
    "sns.violinplot(x = \"Pclass\", y = \"Age\", hue = 'Survived', data=data, split=True, bw = 0.05 , palette = PALETTE, ax = ax)\n",
    "plt.title('Survival Distributions by Age and Class ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the catplot function (in just one line of code!) to show the comparable distributions for Class, Age, Sex and Survived\n",
    "\n",
    "g = sns.catplot(x = \"Pclass\", y = \"Age\", hue = \"Survived\", col = \"Sex\", data = data, kind = \"violin\", split = True, bw = 0.05, palette = PALETTE, height = 7, aspect = 0.9, s = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4 (a glimpse of ML): Predict Survival\n",
    "Now that we are done with our exploratory data analysis, we can ask the following supervised machine learning problem: can we learn a model that predicts which passengers will live or die, by using the other passengers' attributes?\n",
    "\n",
    "This problem is perhaps a bit silly, because we of course already who lived or died now. One could argue that it could still be an interesting proxy to quantify the probability of any passenger to die if another similar boat were to have an accident at that time. In practical contexts, attacking such a problem would require a clear goal and motivation. But for us it's mostly a good pretext to scratch the surface of machine learning :) Also, if you find a really good model, you can try submitting solution to the [Kaggle challenge](https://www.kaggle.com/c/titanic)! You probably will not win it, because the leaderboard there is populated only by overfitting models - but that is another topic.\n",
    "\n",
    "Our go-to library here will be `sklearn`, which is a great ML library that contains many useful models and procedures that are very often used. You can consult the [documentation](https://scikit-learn.org/stable/user_guide.html).\n",
    "\n",
    "### 4.1 Feature Selection\n",
    "Let's now decide which features we keep. There are automated ways to do that, but here we'll just do it manually.\n",
    "Using the insights we got above, it seems that both the passenger class and the gender were important. Let's also add the fare:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We keep only these two columns:\n",
    "X = data[['Pclass', 'Sex', 'Fare']]\n",
    "\n",
    "# In addition, we also keep track of our target variable (in an array):\n",
    "y = data['Survived'].values\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Feature Preprocessing\n",
    "First, let's encode the gender using numerical values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to transform our categorical \"Sex\" variable into 0-1 coded values.\n",
    "X = pd.get_dummies(X, drop_first=True)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, let's normalize the Pclass and Fare columns so that their values are comprised between 0 and 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "X_transformed = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`X_transformed` is now an array, and not a DataFrame anymore. That's OK though - we won't need DataFrame features from this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_transformed.shape)\n",
    "print(y.shape)\n",
    "print(X_transformed[:20,0])\n",
    "print(X_transformed[:20,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Split in Train and Validation Set\n",
    "We start by splitting our original \"training\" set into a training and a validation set. This is to ensure that we don't evaluate our future model on the same data that have been used for training it. We use the validations set instead for that. Usually, we also keep a test set entirely separate, which we only use once we have a final model, in order to estimate its final accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split our dataframe, keeping 30% for the validation set:\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_transformed, y, test_size=0.30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Fit a Simple Model\n",
    "Here, we'll first try a simple logistic regression. That's often a great first model to try for such classification problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Fit our model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a prediction on the validation set\n",
    "y_hat = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_accuracy(y_predicted, y_true):\n",
    "    assert y_predicted.shape == y_true.shape\n",
    "    print('Accuracy: %.3f' % (sum(y_predicted == y_true) / len(y_true)))\n",
    "print_accuracy(y_hat, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Trying a More Fancy Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Fit our model\n",
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a prediction on the validation set\n",
    "y_hat = model_rf.predict(X_val)\n",
    "print_accuracy(y_hat, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can you do better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
